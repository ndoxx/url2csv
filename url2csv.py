#!/usr/bin/python3

from urllib.parse import urlencode
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
from pathlib import Path

import csv, sys, argparse, locale
from bs4 import BeautifulSoup
from common import bcolors


verbose = False


def vprint(message):
	if verbose:
		print('# ' + message)


def verror(message):
	if verbose:
		print('# ' + message)
	print(bcolors.RED + message + bcolors.ENDC, file=sys.stderr)


def get_page(url):
	headers = {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
			   'dnt': '1',
			   'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',
			   }
	req = Request(url, data=None, headers=headers, method='GET')
	try:
		response = urlopen(req, timeout=1).read().decode()
		return response
		
	except URLError as e:
		verror(str(e))
		sys.exit(2)
	except HTTPError as e:
		verror(str(e))
		sys.exit(2)
	except Exception as e:
		verror(str(e))
		sys.exit(2)


def cast(element, fmt):
	if fmt == 's':
		return element
	elif fmt == 'i':
		return locale.atoi(element)
	elif fmt == 'f':
		return locale.atof(element)
	else:
		return element


def main(argv):
	loc = locale.getlocale()
	loc_str = '.'.join(loc)

	parser = argparse.ArgumentParser()
	parser.add_argument('url', help='The URL to extract the table from')
	parser.add_argument('-f', '--format', default='', help='Format string describing each column. Can contain:\n ~(ignore) i(int) f(float) s(string)')
	parser.add_argument('-v', '--verbose', action='store_true', help='Export warning/error messages as CSV comments')
	parser.add_argument('-l', '--locale', default=loc_str, help='Select the locale (defaults to system settings)')
	args = parser.parse_args()

	global verbose
	verbose = args.verbose

	vprint('Generated by url2csv.py')
	vprint('Using locale: ' + loc_str)

	locale.setlocale(locale.LC_ALL, loc_str)

	# Get page as string, parse it, extract table
	# NOTE(ndx): ASSUME single table for now
	p = get_page(args.url)

	soup = BeautifulSoup(p, 'lxml')
	data = []
	table = soup.find('table')
	table_body = table.find('tbody')

	rows = table_body.find_all('tr')
	for row in rows:
		cols = row.find_all('td')
		cols = [ele.text.strip() for ele in cols]
		# Get rid of empty rows
		if len(cols):
			data.append(cols)

	# Default format string
	ncols = len(data[0])
	if len(args.format) != ncols:
		args.format = 's' * ncols
		verror('Defaulting to string format for each column.')

	# Filter, format and output data
	for row in data:
		xrow = [cast(ele,args.format[ii]) for ii, ele in enumerate(row) if args.format[ii] != '~']
		print(';'.join(map(str, xrow)))


if __name__ == '__main__':
    main(sys.argv[1:])